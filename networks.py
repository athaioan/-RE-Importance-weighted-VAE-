import torch
import numpy as np
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.autograd import Variable
import time
import os
from sklearn.decomposition import PCA


class VAE_1(nn.Module):
    class Encoder(nn.Module):
        ## encoder
        ## q(z|x) estimated posterior

        def __init__(self, latent_dim=50, hidden_dim=200, input_dim=784):
            super(VAE_1.Encoder, self).__init__()

            self.tanh_activation = nn.Tanh()
            self.deterministic_1 = nn.Linear(input_dim, hidden_dim)
            self.deterministic_2 = nn.Linear(hidden_dim, hidden_dim)
            self.stohastic_layer_mean = nn.Linear(hidden_dim, latent_dim)
            self.stohastic_layer_log_sigma = nn.Linear(hidden_dim, latent_dim)

        def forward(self, x):
            x = self.deterministic_1(x)
            x = self.tanh_activation(x)

            x = self.deterministic_2(x)
            x = self.tanh_activation(x)

            latent_mean = self.stohastic_layer_mean(x)
            latent_log_sigma = self.stohastic_layer_log_sigma(x)

            # enforces positive sigma in a stable way as explained in
            # https://stats.stackexchange.com/questions/353220/why-in-variational-auto-encoder-gaussian-variational-family-we-model-log-sig
            latent_sigma = torch.exp(latent_log_sigma)  # sigma = exp(log_sigma)

            return latent_mean, latent_sigma

    class Decoder(nn.Module):
        ## decoder
        ## p(x|z) estimated posterior

        def __init__(self, latent_dim=50, hidden_dim=200, input_dim=784):
            super(VAE_1.Decoder, self).__init__()

            self.tanh_activation = nn.Tanh()
            self.sigmoid_activation = nn.Sigmoid()

            self.deterministic_1 = nn.Linear(latent_dim, hidden_dim)
            self.deterministic_2 = nn.Linear(hidden_dim, hidden_dim)
            self.x_hat_layer = nn.Linear(hidden_dim, input_dim)

        def forward(self, x):
            x = self.deterministic_1(x)
            x = self.tanh_activation(x)

            x = self.deterministic_2(x)
            x = self.tanh_activation(x)

            x = self.x_hat_layer(x)
            x_hat = self.sigmoid_activation(x)

            return x_hat

    def __init__(self, latent_dim=50, hidden_dim=200, input_dim=784, IW=False, k=1, device="cpu"):
        super(VAE_1, self).__init__()

        self.train_loss = []
        self.current_epoch = 0
        self.latent_dim = 50
        self.device = device
        self.IW = IW
        self.k = k
        self.encoder = self.Encoder(latent_dim=latent_dim, hidden_dim=hidden_dim, input_dim=input_dim)
        self.decoder = self.Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, input_dim=input_dim)

        self.to(self.device)
        self.double()

    def forward(self, x):

        latent_mean, latent_sigma = self.encoder(x)

        ## eps ~ N(0,1)
        eps = torch.normal(0, 1, size=latent_sigma.size()).to(self.device)

        ## reparameterization trick
        ## sampling z_latent
        z_latent = eps * latent_sigma + latent_mean

        ## decoding
        x_hat = self.decoder(z_latent)

        return latent_mean, latent_sigma, z_latent, x_hat

    def train_epoch(self, train_loader, optimizer):

        self.train()

        overall_loss = 0
        for index, data in enumerate(train_loader):

            input_imgs = Variable(data.double().to(self.device))

            if self.IW:
                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(self.k, 1, 1)

                # input_labels = input_labels.unsqueeze(0)
                # input_labels = input_labels.repeat(self.k, 1)
            else:
                input_imgs = input_imgs.repeat(self.k, 1)
                input_imgs = input_imgs.unsqueeze(0)

                # input_labels = input_labels.repeat(self.k)

            latent_mean, latent_sigma, z_latent, x_hat = self(input_imgs)

            ## ELBO_RE (ELBO reconstruction error part)
            ## x being a bernuli distribution
            ELBO_RE = torch.sum(input_imgs * torch.log(x_hat) + (1 - input_imgs) * torch.log(1 - x_hat), dim=2)

            log_z_prior = torch.sum(-0.5 * torch.pow(z_latent, 2),
                                    dim=2)  ## loglikelihood of z_prior being generated by (0,1) Normal Distribution ## summing over the

            log_q_z = torch.sum(
                -0.5 * torch.pow(((z_latent - latent_mean) / latent_sigma), 2) - torch.log(latent_sigma), dim=2)

            ## loglikelihood of z_prior being generated by predicted Normal Distributions

            ELBO_KL = (log_q_z - log_z_prior)

            ELBO = ELBO_RE - ELBO_KL

            ## weights calculation (weight normalization based on the log_sum trick)
            ## based on https://github.com/xqding/Importance_Weighted_Autoencoders and the
            ## equation [13] of target paper
            log_weights = ELBO
            log_weights = log_weights - torch.max(log_weights, dim=0)[0]

            weights = torch.exp(log_weights)
            weights /= torch.sum(weights, dim=0)

            weights = Variable(weights.data, requires_grad=False)  ## the weights shall not be trainable
            # Note weightes = ones in cases of VAE

            IW_ELBO = torch.sum(weights * ELBO, dim=0)

            loss = - torch.mean(IW_ELBO)  # averaging across all samples of the batch

            overall_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print('Train Epoch: {}/{}\n'
                  'Step: {}/{}\n'
                  'Batch ~ Loss: {:.4f}\n'.format(self.current_epoch + 1, self.epochs,
                                                  index + 1, len(train_loader),
                                                  loss.data.cpu().numpy()))

        self.current_epoch += 1
        self.train_loss.append(overall_loss / len(train_loader))

    def test_nll(self, test_loader):

        self.eval()
        total_nll = 0
        active_latent = []
        with torch.no_grad():
            for index, data in enumerate(test_loader):
                input_imgs = data.double().to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(5000, 1, 1)

                latent_mean, latent_sigma, z_latent, x_hat = self(input_imgs)

                active_latent.append(torch.mean(latent_mean, dim=0)[0].data.cpu().numpy())
                ## ELBO_RE (ELBO reconstruction error part)
                ## x being a bernuli distribution
                ELBO_RE = torch.sum(input_imgs * torch.log(x_hat) + (1 - input_imgs) * torch.log(1 - x_hat), dim=2)

                log_z_prior = torch.sum(-0.5 * torch.pow(z_latent, 2),
                                        dim=2)  ## loglikelihood of z_prior being generated by (0,1) Normal Distribution ## summing over the

                log_q_z = torch.sum(
                    -0.5 * torch.pow(((z_latent - latent_mean) / latent_sigma), 2) - torch.log(latent_sigma), dim=2)

                ## loglikelihood of z_prior being generated by predicted Normal Distributions

                ELBO_KL = (log_q_z - log_z_prior)

                ELBO = ELBO_RE - ELBO_KL

                ## weights calculation (weight normalization based on the log_sum trick)
                ## based on https://github.com/xqding/Importance_Weighted_Autoencoders and the
                ## equation [13] of target paper
                nll_all = ELBO

                nll_all_exp = torch.exp(nll_all)
                nll = - torch.log(torch.mean(nll_all_exp, dim=0))

                total_nll += nll

                print('Step: {}/{}\n'.format(index + 1, len(test_loader)))

            active_units = np.sum(np.diag(np.cov(np.asarray(active_latent).T)) > 1e-2)
            print(active_units)

            return total_nll / len(test_loader)

    def generate(self, N, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        z_latent = torch.normal(0, 1, size=(N, self.latent_dim)).to(self.device).double()

        output_means = self.decoder(z_latent)
        generated_images = output_means.data.cpu().numpy().reshape(output_means.shape[0], 28, 28)

        for index in range(generated_images.shape[0]):
            current_image = generated_images[index]
            plt.figure()
            plt.imshow(current_image, cmap='gray')
            plt.axis('off')
            plt.savefig(save_fold + str(index) + ".png", bbox_inches='tight', transparent=True, pad_inches=0)
            plt.close()

    def reconstruct(self, test_loader, N, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        self.eval()

        with torch.no_grad():
            for index, data in enumerate(test_loader):

                if index > N:
                    break

                input_imgs = data.double().to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(1, 1, 1)

                latent_mean, latent_sigma, z_latent, x_hat = self(input_imgs)

                input_image = input_imgs[0, 0].data.cpu().numpy().reshape(28, 28)
                reconstructed_image = x_hat[0, 0].data.cpu().numpy().reshape(28, 28)

                plt.figure()
                plt.imshow(input_image, cmap='gray')
                plt.axis('off')
                plt.savefig(save_fold + str(index) + "_input.png", bbox_inches='tight', transparent=True, pad_inches=0)
                plt.close()

                plt.figure()
                plt.imshow(reconstructed_image, cmap='gray')
                plt.axis('off')
                plt.savefig(save_fold + str(index) + "_reconstruct.png", bbox_inches='tight', transparent=True,
                            pad_inches=0)
                plt.close()

    def plot_latent(self, test_loader_labels, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        self.eval()

        latens = []
        labels = []
        with torch.no_grad():
            for index, data in enumerate(test_loader_labels):
                input_imgs = data[0].double().to(self.device)
                label = data[1].to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(1, 1, 1)

                latent_mean, latent_sigma, z_latent, x_hat = self(input_imgs)
                latens.append(latent_mean[0, 0].data.cpu().numpy())
                labels.append(label[0].data.cpu().numpy())

        latens = np.asarray(latens)
        labels = np.asarray(labels)
        pca = PCA(n_components=2, svd_solver='full')
        latents_pca = pca.fit(latens.T).components_

        fig, ax = plt.subplots()
        for g in np.arange(10):
            ix = np.where(labels == g)
            ax.scatter(latents_pca[0, ix], latents_pca[1, ix], label=g, s=100)
        ax.legend()
        plt.axis('off')
        plt.savefig(save_fold + str(index) + ".png", bbox_inches='tight', transparent=True, pad_inches=0)
        plt.close()


class VAE_2(nn.Module):
    class Encoder(nn.Module):
        ## encoder
        ## q(z|x) estimated posterior

        def __init__(self, latent_dim_1=100, latent_dim_2=50, hidden_dim_1=200, hidden_dim_2=100, input_dim=784,
                     device="cuda"):
            super(VAE_2.Encoder, self).__init__()

            self.device = device
            self.tanh_activation = nn.Tanh()
            ## first stohastic layer
            self.deterministic_1_1 = nn.Linear(input_dim, hidden_dim_1)
            self.deterministic_1_2 = nn.Linear(hidden_dim_1, hidden_dim_1)
            self.stohastic_layer_mean_1 = nn.Linear(hidden_dim_1, latent_dim_1)
            self.stohastic_layer_log_sigma_1 = nn.Linear(hidden_dim_1, latent_dim_1)

            ## second stohastic layer
            self.deterministic_2_1 = nn.Linear(latent_dim_1, hidden_dim_2)
            self.deterministic_2_2 = nn.Linear(hidden_dim_2, hidden_dim_2)
            self.stohastic_layer_mean_2 = nn.Linear(hidden_dim_2, latent_dim_2)
            self.stohastic_layer_log_sigma_2 = nn.Linear(hidden_dim_2, latent_dim_2)

        def forward(self, x):
            x = self.deterministic_1_1(x)
            x = self.tanh_activation(x)

            x = self.deterministic_1_2(x)
            x = self.tanh_activation(x)

            latent_mean_1 = self.stohastic_layer_mean_1(x)
            latent_log_sigma_1 = self.stohastic_layer_log_sigma_1(x)

            # enforces positive sigma in a stable way as explained in
            # https://stats.stackexchange.com/questions/353220/why-in-variational-auto-encoder-gaussian-variational-family-we-model-log-sig
            latent_sigma_1 = torch.exp(latent_log_sigma_1)  # sigma = exp(log_sigma)

            ## eps ~ N(0,1)
            eps = torch.normal(0, 1, size=latent_sigma_1.size()).to(self.device)
            ## reparameterization trick
            ## sampling z_latent
            z_latent_1 = eps * latent_sigma_1 + latent_mean_1

            x = self.deterministic_2_1(z_latent_1)
            x = self.tanh_activation(x)

            x = self.deterministic_2_2(x)
            x = self.tanh_activation(x)

            latent_mean_2 = self.stohastic_layer_mean_2(x)
            latent_log_sigma_2 = self.stohastic_layer_log_sigma_2(x)
            latent_sigma_2 = torch.exp(latent_log_sigma_2)  # sigma = exp(log_sigma)

            return latent_mean_1, latent_sigma_1, z_latent_1, latent_mean_2, latent_sigma_2

    class Decoder(nn.Module):
        ## decoder
        ## p(x|z) estimated posterior

        def __init__(self, latent_dim_1=100, latent_dim_2=50, hidden_dim_1=200, hidden_dim_2=100, input_dim=784,
                     device="cuda"):
            super(VAE_2.Decoder, self).__init__()

            self.device = device
            self.tanh_activation = nn.Tanh()
            self.sigmoid_activation = nn.Sigmoid()

            ## first stohastic layer
            self.deterministic_1_1 = nn.Linear(latent_dim_2, hidden_dim_2)
            self.deterministic_1_2 = nn.Linear(hidden_dim_2, hidden_dim_2)

            self.latent_hat_mean_1 = nn.Linear(hidden_dim_2, latent_dim_1)
            self.latent_log_sigma_1 = nn.Linear(hidden_dim_2, latent_dim_1)

            ## first stohastic layer
            self.deterministic_2_1 = nn.Linear(latent_dim_1, hidden_dim_1)
            self.deterministic_2_2 = nn.Linear(hidden_dim_1, hidden_dim_1)

            self.x_hat_layer = nn.Linear(hidden_dim_1, input_dim)

        def forward(self, z_latent_1, z_latent_2, generate=False):
            x = self.deterministic_1_1(z_latent_2)
            x = self.tanh_activation(x)

            x = self.deterministic_1_2(x)
            x = self.tanh_activation(x)

            latent_mean_hat_1 = self.latent_hat_mean_1(x)
            latent_log_sigma_1 = self.latent_log_sigma_1(x)
            latent_sigma_hat_1 = torch.exp(latent_log_sigma_1)  # sigma = exp(log_sigma)

            if generate:
                ## eps ~ N(0,1)
                eps = torch.normal(0, 1, size=latent_sigma_hat_1.size()).to(self.device)
                ## reparameterization trick
                ## sampling z_latent
                z_latent_1 = eps * latent_sigma_hat_1 + latent_mean_hat_1

            x = self.deterministic_2_1(z_latent_1)
            x = self.tanh_activation(x)

            x = self.deterministic_2_2(x)
            x = self.tanh_activation(x)

            x = self.x_hat_layer(x)
            x_hat = self.sigmoid_activation(x)

            return z_latent_1, latent_mean_hat_1, latent_sigma_hat_1, x_hat

    def __init__(self, latent_dim_1=100, latent_dim_2=50, hidden_dim_1=200, hidden_dim_2=100, input_dim=784, IW=False,
                 k=1, device="cpu"):

        super(VAE_2, self).__init__()

        self.train_loss = []
        self.current_epoch = 0
        self.latent_dim_1 = latent_dim_1
        self.latent_dim_2 = latent_dim_2
        self.device = device
        self.IW = IW
        self.k = k
        self.encoder = self.Encoder(latent_dim_1=latent_dim_1, latent_dim_2=latent_dim_2, hidden_dim_1=hidden_dim_1,
                                    hidden_dim_2=hidden_dim_2,
                                    input_dim=input_dim)

        self.decoder = self.Decoder(latent_dim_1=latent_dim_1, latent_dim_2=latent_dim_2, hidden_dim_1=hidden_dim_1,
                                    hidden_dim_2=hidden_dim_2,
                                    input_dim=input_dim)

        self.to(self.device)
        self.double()

    def forward(self, x):

        latent_mean_1, latent_sigma_1, z_latent_1, latent_mean_2, latent_sigma_2 = self.encoder(x)

        ## eps ~ N(0,1)
        eps = torch.normal(0, 1, size=latent_sigma_2.size()).to(self.device)

        ## reparameterization trick
        ## sampling z_latent_2
        z_latent_2 = eps * latent_sigma_2 + latent_mean_2

        ## decoding
        z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat = self.decoder(z_latent_1, z_latent_2)

        return z_latent_1, latent_mean_1, latent_sigma_1, \
               z_latent_2, latent_mean_2, latent_sigma_2, \
               z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat

    def train_epoch(self, train_loader, optimizer):

        self.train()

        overall_loss = 0
        for index, data in enumerate(train_loader):

            input_imgs = Variable(data.double().to(self.device))

            if self.IW:
                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(self.k, 1, 1)

            else:
                input_imgs = input_imgs.repeat(self.k, 1)
                input_imgs = input_imgs.unsqueeze(0)

            z_latent_1, latent_mean_1, latent_sigma_1, \
            z_latent_2, latent_mean_2, latent_sigma_2, \
            z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat = self(input_imgs)

            ## ELBO_RE (ELBO reconstruction error part)
            ## x being a bernuli distribution
            ELBO_RE_x = torch.sum(input_imgs * torch.log(x_hat) + (1 - input_imgs) * torch.log(1 - x_hat), dim=2)
            ELBO_RE_latent = torch.sum(
                -0.5 * torch.pow(((z_latent_1_decoder - latent_mean_hat_1) / latent_sigma_hat_1), 2)
                - torch.log(latent_sigma_hat_1), dim=2)

            log_z_prior_2 = torch.sum(-0.5 * torch.pow(z_latent_2, 2),
                                      dim=2)  ## loglikelihood of z_prior being generated by (0,1) Normal Distribution ## summing over the
            log_q_z_1 = torch.sum(
                -0.5 * torch.pow(((z_latent_1 - latent_mean_1) / latent_sigma_1), 2) - torch.log(latent_sigma_1), dim=2)
            log_q_z_2 = torch.sum(
                -0.5 * torch.pow(((z_latent_2 - latent_mean_2) / latent_sigma_2), 2) - torch.log(latent_sigma_2), dim=2)

            ## loglikelihood of z_prior being generated by predicted Normal Distributions

            ELBO_KL = (log_q_z_1 + log_q_z_2 - log_z_prior_2)

            ELBO = ELBO_RE_x + ELBO_RE_latent - ELBO_KL

            # ELBO_KL = (log_q_z - log_z_prior)
            #
            # ELBO = ELBO_RE - ELBO_KL

            ## weights calculation (weight normalization based on the log_sum trick)
            ## based on https://github.com/xqding/Importance_Weighted_Autoencoders and the
            ## equation [13] of target paper
            log_weights = ELBO
            log_weights = log_weights - torch.max(log_weights, dim=0)[0]

            weights = torch.exp(log_weights)
            weights /= torch.sum(weights, dim=0)

            weights = Variable(weights.data, requires_grad=False)  ## the weights shall not be trainable
            # Note weightes = ones in cases of VAE

            IW_ELBO = torch.sum(weights * ELBO, dim=0)

            loss = - torch.mean(IW_ELBO)  # averaging across all samples of the batch

            overall_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print('Train Epoch: {}/{}\n'
                  'Step: {}/{}\n'
                  'Batch ~ Loss: {:.4f}\n'.format(self.current_epoch + 1, self.epochs,
                                                  index + 1, len(train_loader),
                                                  loss.data.cpu().numpy()))

        self.current_epoch += 1
        self.train_loss.append(overall_loss / len(train_loader))

    def test_nll(self, test_loader):

        self.eval()
        total_nll = 0
        active_latent_1 = []
        active_latent_2 = []

        with torch.no_grad():
            for index, data in enumerate(test_loader):
                input_imgs = data.double().to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(5, 1, 1)

                z_latent_1, latent_mean_1, latent_sigma_1, \
                z_latent_2, latent_mean_2, latent_sigma_2, \
                z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat = self(input_imgs)

                active_latent_1.append(torch.mean(latent_mean_1, dim=0)[0].data.cpu().numpy())
                active_latent_2.append(torch.mean(latent_mean_2, dim=0)[0].data.cpu().numpy())

                ## ELBO_RE (ELBO reconstruction error part)
                ## x being a bernuli distribution
                ## ELBO_RE (ELBO reconstruction error part)
                ## x being a bernuli distribution
                ELBO_RE_x = torch.sum(input_imgs * torch.log(x_hat) + (1 - input_imgs) * torch.log(1 - x_hat), dim=2)
                ELBO_RE_latent = torch.sum(
                    -0.5 * torch.pow(((z_latent_1_decoder - latent_mean_hat_1) / latent_sigma_hat_1), 2)
                    - torch.log(latent_sigma_hat_1), dim=2)

                log_z_prior_2 = torch.sum(-0.5 * torch.pow(z_latent_2, 2),
                                          dim=2)  ## loglikelihood of z_prior being generated by (0,1) Normal Distribution ## summing over the
                log_q_z_1 = torch.sum(
                    -0.5 * torch.pow(((z_latent_1 - latent_mean_1) / latent_sigma_1), 2) - torch.log(latent_sigma_1),
                    dim=2)
                log_q_z_2 = torch.sum(
                    -0.5 * torch.pow(((z_latent_2 - latent_mean_2) / latent_sigma_2), 2) - torch.log(latent_sigma_2),
                    dim=2)

                ## loglikelihood of z_prior being generated by predicted Normal Distributions

                ELBO_KL = (log_q_z_1 + log_q_z_2 - log_z_prior_2)

                ELBO = ELBO_RE_x + ELBO_RE_latent - ELBO_KL

                nll_all = ELBO

                nll_all_exp = torch.exp(nll_all)
                nll = - torch.log(torch.mean(nll_all_exp, dim=0))

                total_nll += nll

                print('Step: {}/{}\n'.format(index + 1, len(test_loader)))

            active_units_1 = np.sum(np.diag(np.cov(np.asarray(active_latent_1).T)) > 1e-2)
            active_units_2 = np.sum(np.diag(np.cov(np.asarray(active_latent_2).T)) > 1e-2)

            print(active_units_1)
            print(active_units_2)

            return total_nll / len(test_loader)

    def generate(self, N, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        z_latent_1 = torch.normal(0, 1, size=(N, self.latent_dim_1)).to(self.device).double()
        z_latent_2 = torch.normal(0, 1, size=(N, self.latent_dim_2)).to(self.device).double()

        _, _, _, output_means = self.decoder(z_latent_1, z_latent_2)

        generated_images = output_means.data.cpu().numpy().reshape(output_means.shape[0], 28, 28)

        for index in range(generated_images.shape[0]):
            current_image = generated_images[index]
            plt.figure()
            plt.imshow(current_image, cmap='gray')
            plt.axis('off')
            plt.savefig(save_fold + str(index) + ".png", bbox_inches='tight', transparent=True, pad_inches=0)
            plt.close()

    def reconstruct(self, test_loader, N, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        self.eval()

        with torch.no_grad():
            for index, data in enumerate(test_loader):

                if index > N:
                    break

                input_imgs = data.double().to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(1, 1, 1)

                z_latent_1, latent_mean_1, latent_sigma_1, \
                z_latent_2, latent_mean_2, latent_sigma_2, \
                z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat = self(input_imgs)

                input_image = input_imgs[0, 0].data.cpu().numpy().reshape(28, 28)
                reconstructed_image = x_hat[0, 0].data.cpu().numpy().reshape(28, 28)

                plt.figure()
                plt.imshow(input_image, cmap='gray')
                plt.axis('off')
                plt.savefig(save_fold + str(index) + "_input.png", bbox_inches='tight', transparent=True, pad_inches=0)
                plt.close()

                plt.figure()
                plt.imshow(reconstructed_image, cmap='gray')
                plt.axis('off')
                plt.savefig(save_fold + str(index) + "_reconstruct.png", bbox_inches='tight', transparent=True,
                            pad_inches=0)
                plt.close()

    def plot_latent(self, test_loader_labels, save_fold):

        if not os.path.exists(save_fold):
            os.makedirs(save_fold)

        self.eval()

        latens = []
        labels = []
        with torch.no_grad():
            for index, data in enumerate(test_loader_labels):
                input_imgs = data[0].double().to(self.device)
                label = data[1].to(self.device)

                input_imgs = input_imgs.unsqueeze(0)
                input_imgs = input_imgs.repeat(1, 1, 1)

                z_latent_1, latent_mean_1, latent_sigma_1, \
                z_latent_2, latent_mean_2, latent_sigma_2, \
                z_latent_1_decoder, latent_mean_hat_1, latent_sigma_hat_1, x_hat = self(input_imgs)

                latens.append(latent_mean_2[0, 0].data.cpu().numpy())
                labels.append(label[0].data.cpu().numpy())

        latens = np.asarray(latens)
        labels = np.asarray(labels)
        pca = PCA(n_components=2, svd_solver='full')
        latents_pca = pca.fit(latens.T).components_

        fig, ax = plt.subplots()
        for g in np.arange(10):
            ix = np.where(labels == g)
            ax.scatter(latents_pca[0, ix], latents_pca[1, ix], label=g, s=100)
        ax.legend()
        plt.axis('off')
        plt.savefig(save_fold + str(index) + ".png", bbox_inches='tight', transparent=True, pad_inches=0)
        plt.close()
